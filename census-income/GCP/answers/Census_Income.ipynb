{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Online Prediction with XGBoost on AI Platform\n",
    "This notebook uses the [Census Income Data Set](https://archive.ics.uci.edu/ml/datasets/Census+Income) to create a simple XGBoost model, upload the model to AI Platform, and query it for predictions. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# How to bring your model to AI Platform\n",
    "Getting your model ready for predictions can be done in 5 steps:\n",
    "1. Save your model to a file\n",
    "2. Upload the saved model to [Google Cloud Storage](https://cloud.google.com/storage)\n",
    "3. Create a model resource on AI Platform\n",
    "4. Create a model version (linking your XGBoost model)\n",
    "5. Make an online prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prerequisites\n",
    "Before we begin, let’s cover some of the different tools you’ll use to get online prediction up and running on AI Platform. \n",
    "\n",
    "[Google Cloud Platform](https://cloud.google.com/) (GCP) lets you build and host applications and websites, store data, and analyze data on Google's scalable infrastructure.\n",
    "\n",
    "[AI Platform](https://cloud.google.com/ml-engine/) is a managed service that enables you to easily build machine learning models that work on any type of data, of any size.\n",
    "\n",
    "[Google Cloud Storage](https://cloud.google.com/storage/) (GCS) is a unified object storage for developers and enterprises, from live data serving to data analytics/ML to data archiving.\n",
    "\n",
    "[Cloud SDK](https://cloud.google.com/sdk/) is a command line tool which allows you to interact with Google Cloud products. In order to run this notebook, make sure that Cloud SDK is [installed](https://cloud.google.com/sdk/downloads) in the same environment as your Jupyter kernel.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 0: Setup\n",
    "* [Create a project on GCP](https://cloud.google.com/resource-manager/docs/creating-managing-projects)\n",
    "* [Create a GCS Bucket](https://cloud.google.com/storage/docs/quickstart-console)\n",
    "* [Enable AI Platform Training and Prediction and Compute Engine APIs](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component&_ga=2.217405014.1312742076.1516128282-1417583630.1516128282)\n",
    "* [Install Cloud SDK](https://cloud.google.com/sdk/downloads)\n",
    "* [Install XGBoost](http://xgboost.readthedocs.io/en/latest/build.html)\n",
    "* [Install scikit-learn](http://scikit-learn.org/stable/install.html)\n",
    "* [Install NumPy](https://docs.scipy.org/doc/numpy/user/install.html)\n",
    "* [Install pandas](https://pandas.pydata.org/pandas-docs/stable/install.html)\n",
    "* [Install Google API Python Client](https://github.com/google/google-api-python-client)\n",
    "\n",
    "\n",
    "These variables will be needed for the exercise.\n",
    "\n",
    "In the cell below, **replace** the following highlighted elements:\n",
    "* `project <PROJECT_ID>` - with this project id (i.e. ai-platform-demo)\n",
    "* `bucket <BUCKET_ID>` - with your student id (i.e. maven-student01)\n",
    "* `folder <FOLDER>` - with something about this exercise (i.e. census_income)\n",
    "* `region <REGION>` - with the correct region (i.e. us-central1) (See: https://cloud.google.com/ai-platform/training/docs/regions)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Replace <PROJECT_ID>, <BUCKET_ID>, and <FOLDER> with proper Project, Bucket ID, and Folder.\n",
    "project = '<PROJECT_ID>'\n",
    "bucket = '<BUCKET_ID>'\n",
    "folder='census-income'\n",
    "region='us-central1'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "bucket_path=f'{bucket}/{folder}'\n",
    "%env PROJECT_ID=$project\n",
    "%env BUCKET_ID=$bucket\n",
    "%env BUCKET_PATH=$bucket_path\n",
    "%env REGION=$region\n",
    "!gsutil mb -c standard -l {region} gs://{bucket}"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "env: PROJECT_ID=ai-fulcrum-admin\n",
      "env: BUCKET_ID=maven-user1\n",
      "env: BUCKET_PATH=maven-user1/census-income\n",
      "env: REGION=us-central1\n",
      "Creating gs://maven-user1/...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download the data\n",
    "The [Census Income Data Set](https://archive.ics.uci.edu/ml/datasets/Census+Income) that this sample\n",
    "uses for training is hosted by the [UC Irvine Machine Learning\n",
    "Repository](https://archive.ics.uci.edu/ml/datasets/):\n",
    "\n",
    " * Training file is `adult.data`\n",
    " * Evaluation file is `adult.test`\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Disclaimer\n",
    "This dataset is provided by a third party. Google provides no representation,\n",
    "warranty, or other guarantees about the validity or any other aspects of this dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Download the data from it's location to your bucket\n",
    "!gsutil cp gs://amazing-public-data/census_income/census_income_data_adult.data gs://${BUCKET_PATH}/adult.data\n",
    "!gsutil cp gs://amazing-public-data/census_income/census_income_data_adult.test gs://${BUCKET_PATH}/adult.test"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Copying gs://amazing-public-data/census_income/census_income_data_adult.data [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  3.8 MiB/  3.8 MiB]                                                \n",
      "Operation completed over 1 objects/3.8 MiB.                                      \n",
      "Copying gs://amazing-public-data/census_income/census_income_data_adult.test [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  1.9 MiB/  1.9 MiB]                                                \n",
      "Operation completed over 1 objects/1.9 MiB.                                      \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.python.lib.io import file_io\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# categorical columns contain data that need to be turned into numerical values before being used by XGBoost\n",
    "CATEGORICAL_COLUMNS = (\n",
    "    \"workclass\",\n",
    "    \"education\",\n",
    "    \"marital-status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"native-country\"\n",
    ")\n",
    "\n",
    "bucket_name=os.environ['BUCKET_PATH']\n",
    "\n",
    "# load training set\n",
    "with file_io.FileIO(f\"gs://{bucket_name}/adult.data\", \"r\") as train_data:\n",
    "    raw_training_data = pd.read_csv(train_data)\n",
    "# remove column we are trying to predict ('income') from features list\n",
    "train_features = raw_training_data.drop(\"income\", axis=1)\n",
    "# create training labels list\n",
    "train_labels = raw_training_data[\"income\"] == \" >50K\"\n",
    "\n",
    "# load test set\n",
    "with file_io.FileIO(f\"gs://{bucket_name}/adult.test\", \"r\") as test_data:\n",
    "    raw_testing_data = pd.read_csv(test_data, skiprows=[1])\n",
    "# remove column we are trying to predict ('income') from features list\n",
    "test_features = raw_testing_data.drop(\"income\", axis=1)\n",
    "# create training labels list\n",
    "test_labels = raw_testing_data[\"income\"] == \" >50K.\"\n",
    "\n",
    "# convert data in categorical columns to numerical values\n",
    "encoders = {col: LabelEncoder() for col in CATEGORICAL_COLUMNS}\n",
    "for col in CATEGORICAL_COLUMNS:\n",
    "    train_features[col] = encoders[col].fit_transform(train_features[col])\n",
    "for col in CATEGORICAL_COLUMNS:\n",
    "    test_features[col] = encoders[col].fit_transform(test_features[col])\n",
    "\n",
    "# For use to verify results as an optional step.\n",
    "data = []\n",
    "for i in range(len(test_features)):\n",
    "    data.append([])\n",
    "    for col in train_features.columns: # ignore 'income' column as it isn't in feature set.\n",
    "        # convert from numpy integers to standard integers\n",
    "        data[i].append(int(np.uint64(test_features[col][i]).item()))\n",
    "\n",
    "# write the test data to a json file\n",
    "with open('test_data.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)\n",
    "    \n",
    "# get one person that makes <=50K and one that makes >50K to test our model.\n",
    "print('Show a person that makes <=50K:')\n",
    "print('\\tFeatures: {0} --> Label: {1}\\n'.format(data[0], test_labels[0]))\n",
    "\n",
    "with open('less_than_50K.json', 'w') as outfile:\n",
    "    json.dump(data[0], outfile)\n",
    "\n",
    "print('Show a person that makes >50K:')\n",
    "print('\\tFeatures: {0} --> Label: {1}'.format(data[2], test_labels[2]))\n",
    "\n",
    "with open('more_than_50K.json', 'w') as outfile:\n",
    "    json.dump(data[2], outfile)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Show a person that makes <=50K:\n",
      "\tFeatures: [25, 4, 226802, 1, 7, 4, 7, 3, 2, 1, 0, 0, 40, 38] --> Label: False\n",
      "\n",
      "Show a person that makes >50K:\n",
      "\tFeatures: [28, 2, 336951, 7, 12, 2, 11, 0, 4, 1, 0, 0, 40, 38] --> Label: True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import xgboost as xgb\n",
    "# load data into DMatrix object\n",
    "dtrain = xgb.DMatrix(train_features, train_labels)\n",
    "dtest = xgb.DMatrix(test_features)\n",
    "\n",
    "# train XGBoost model\n",
    "bst = xgb.train({'objective':'reg:logistic'}, dtrain, num_boost_round=20)\n",
    "bst.save_model('./model.bst')\n",
    "\n",
    "print('model trained and saved')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "model trained and saved\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that the model has been saved locally, let's run it on the first (less than 50k) and third (greater than 50k) data elements in the array."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "bst.predict(dtest)[[0, 2]]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.00463351, 0.34860396], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1: Submit a Local Job to Train/Save a Model and Make a Prediction\n",
    "Local jobs are generally used for debugging purposes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create your Python model file\n",
    "We have created the Python model file (inside trainer folder) that we'll upload to AI Platform. This is similar to your normal process for creating an XGBoost model. However, there are a few key differences:\n",
    "1. Downloading the data from GCS at the start of your file, so that AI Platform can access the data.\n",
    "1. Exporting/saving the model to GCS at the end of your file, so that you can use it for predictions.\n",
    "1. Define a command-line argument in your main training module for AI Platform parameters\n",
    "\n",
    "The code in this file first handles the parameters passed to the file from AI Platform. Then it loads the data into a pandas DataFrame that can be used by XGBoost. Then the model is fit against the training data. Lastly, the model is saved to a file that can be uploaded to [AI Platform's prediction service](https://cloud.google.com/ml-engine/docs/scikit/getting-predictions#deploy_models_and_versions).\n",
    "\n",
    "Note: In normal practice you would want to test your model locally on a small dataset to ensure that it works, before using it with your larger dataset on AI Platform. This avoids wasted time and costs. This is displayed below, as well."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train and Save the Model\n",
    "First, the data is loaded into a pandas DataFrame. Then a simple model is created with the training set. Lastly, the model is saved to a .bst file that can then be uploaded to AI Platform."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "!gcloud ai-platform local train \\\n",
    "  --package-path trainer \\\n",
    "  --module-name trainer.task \\\n",
    "  -- \\\n",
    "  --bucket_name $BUCKET_PATH"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Copying file://model.bst [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 63.0 KiB/ 63.0 KiB]                                                \n",
      "Operation completed over 1 objects/63.0 KiB.                                     \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Data Preparation\n",
    "Before you begin predicting , you'll need to take some of the test data and prepare it, so that the test data can be used by the deployed model.\n",
    "\n",
    "To get predictions, the data needs to be converted from a numpy array to a json array."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Make Predictions Using the Saved Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### This tests for \"greater than 50K\" on a record, which has a person making less than 50K."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "!gcloud ai-platform local predict \\\n",
    "  --model-dir gs://${BUCKET_PATH}/model/ \\\n",
    "  --json-instances less_than_50K.json \\\n",
    "  --framework xgboost \\\n",
    "  --signature-name 'census_income_model'\n",
    "#   --verbosity debug"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Copying gs://maven-user1/census-income/model/model.bst...\n",
      "/ [1 files][ 63.0 KiB/ 63.0 KiB]                                                \n",
      "Operation completed over 1 objects/63.0 KiB.                                     \n",
      "Copying gs://maven-user1/census-income/model/model.bst...\n",
      "/ [1 files][ 63.0 KiB/ 63.0 KiB]                                                \n",
      "Operation completed over 1 objects/63.0 KiB.                                     \n",
      "\n",
      "[0.004633505828678608]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### This tests for \"greater than 50K\" on a record, which has a person making more than 50K."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "!gcloud ai-platform local predict \\\n",
    "  --model-dir gs://${BUCKET_PATH}/model/ \\\n",
    "  --json-instances more_than_50K.json \\\n",
    "  --framework xgboost \\\n",
    "  --signature-name 'census_income_model'\n",
    "#   --verbosity debug"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Copying gs://maven-user1/census-income/model/model.bst...\n",
      "/ [1 files][ 63.0 KiB/ 63.0 KiB]                                                \n",
      "Operation completed over 1 objects/63.0 KiB.                                     \n",
      "Copying gs://maven-user1/census-income/model/model.bst...\n",
      "/ [1 files][ 63.0 KiB/ 63.0 KiB]                                                \n",
      "Operation completed over 1 objects/63.0 KiB.                                     \n",
      "\n",
      "[0.3486039638519287]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 2: Create a job and run training on AI Platform\n",
    "Next we need to create a job for training on AI Platform. We'll use gcloud to submit the job which has the following flags:\n",
    "\n",
    "* `job-name` - A name to use for the job (mixed-case letters, numbers, and underscores only, starting with a letter). In this case: `census_income_job_$(date +\"%Y%m%d_%H%M%S\")`\n",
    "* `job-dir` - The path to a Google Cloud Storage location to use for job output.\n",
    "* `package-path` - A packaged training application that is staged in a Google Cloud Storage location. If you are using the gcloud command-line tool, this step is largely automated.\n",
    "* `module-name` - The name of the main module in your trainer package. The main module is the Python file you call to start the application. If you use the gcloud command to submit your job, specify the main module name in the --module-name argument. Refer to Python Packages to figure out the module name.\n",
    "* `region` - The Google Cloud Compute region where you want your job to run. You should run your training job in the same region as the Cloud Storage bucket that stores your training data. Select a region from [here](https://cloud.google.com/ml-engine/docs/regions) or use the default '`us-central1`'.\n",
    "* `runtime-version` - The version of AI Platform to use for the job. If you don't specify a runtime version, the training service uses the default AI Platform runtime version 1.0. See the list of runtime versions for more information.\n",
    "* `python-version` - The Python version to use for the job. Python 3.5 is available with runtime version 1.4 or greater. If you don't specify a Python version, the training service uses Python 2.7.\n",
    "* Custom parameters used in the Python file\n",
    "\n",
    "\n",
    "Note: Check to make sure gcloud is set to the current PROJECT_ID"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "%env PACKAGE_PATH=trainer\n",
    "%env MODULE_NAME=trainer.task\n",
    "%env RUNTIME_VERSION=2.1\n",
    "%env PYTHON_VERSION=3.7"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "env: PACKAGE_PATH=trainer\n",
      "env: MODULE_NAME=trainer.task\n",
      "env: RUNTIME_VERSION=2.1\n",
      "env: PYTHON_VERSION=3.7\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "now=(datetime.now() + timedelta(hours=-5)).strftime(\"%Y%m%d_%H%M%S\") # Central Time\n",
    "%env JOB_NAME=census_income_job_{now}\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME  \\\n",
    "  --job-dir gs://${BUCKET_PATH}/jobdir \\\n",
    "  --package-path $PACKAGE_PATH \\\n",
    "  --module-name $MODULE_NAME \\\n",
    "  --region $REGION \\\n",
    "  --runtime-version $RUNTIME_VERSION \\\n",
    "  --python-version $PYTHON_VERSION \\\n",
    "  -- \\\n",
    "  --bucket_name $BUCKET_PATH\n",
    "\n",
    "# Model should exit with status \"SUCCEEDED\"\n",
    "cmd = 'gcloud ai-platform jobs describe $JOB_NAME --format=\"value(state)\"'\n",
    "for i in range(20):\n",
    "    time.sleep(10)\n",
    "    !{cmd}"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "env: JOB_NAME=census_income_job_20201113_160033\n",
      "Job [census_income_job_20201113_160033] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe census_income_job_20201113_160033\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs census_income_job_20201113_160033\n",
      "jobId: census_income_job_20201113_160033\n",
      "state: QUEUED\n",
      "PREPARING\n",
      "PREPARING\n",
      "PREPARING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "RUNNING\n",
      "SUCCEEDED\n",
      "SUCCEEDED\n",
      "SUCCEEDED\n",
      "SUCCEEDED\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 3: Upload your model to Google Cloud Storage\n",
    "To use your model with AI Platform, it needs to be uploaded to Google Cloud Storage (GCS). When the state reached \"SUCCESS\" above, the model (model.bst) was copied to *student bucket*/census-income/model. (Check out the /trainer/task.py for details.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: The exact file name of of the exported model you upload to GCS is important! Your model must be named “model.joblib” for sklearn, “model.pkl” for custom prediction routines, or “model.bst” for XGBoost. This restriction ensures that the model will be safely reconstructed later by using the same technique for import as was used during export."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we the PROJECT ID for the *gcloud* command line utility for the next step."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 4: Create a model resource\n",
    "AI Platform organizes your trained models using model and version resources. An AI Platform model is a container for the versions of your machine learning model. For more information on model resources and model versions look [here](https://cloud.google.com/ml-engine/docs/deploying-models#creating_a_model_version). \n",
    "\n",
    "At this step, you create a container that you can use to hold several different versions of your actual model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "These variables will be needed for the remaining steps of the exercise.\n",
    "\n",
    "In the cell below, **replace** the following highlighted elements:\n",
    "\n",
    "* `<YOUR_MODEL_NAME>` - with your model name, such as \"census\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# model_name = '<MODEL_NAME>'\n",
    "model_name = 'census_income_student_3'\n",
    "%env MODEL_NAME=$model_name\n",
    "%env FRAMEWORK=xgboost"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "env: MODEL_NAME=census_income_student_3\n",
      "env: FRAMEWORK=xgboost\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "! gcloud ai-platform models create $MODEL_NAME --regions $REGION"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Created ml engine model [projects/ai-fulcrum-admin/models/census_income_student_3].\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 5: Create a model version"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the cell below, **replace** the following highlighted elements:\n",
    "\n",
    "* `<YOUR_VERSION>` - with your version name, such as \"v1\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "%env VERSION_NAME=v1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "env: VERSION_NAME=v1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now it’s time to get your model online and ready for predictions. The model version requires a few components as specified [here](https://cloud.google.com/ml-engine/reference/rest/v1/projects.models.versions#Version).\n",
    "\n",
    "* __version_name__ - The name specified for the version when it was created. This will be the `VERSION_NAME` variable you declared at the beginning.\n",
    "* __origin__ - Is where the trained model is located in Google Cloud Storage\n",
    "* __runtime version__ - The Google Cloud ML runtime version to use for this deployment.\n",
    "* __framework__ - The framework specifies if you are using: `TENSORFLOW`, `SCIKIT_LEARN`, `XGBOOST`. This is set to `XGBOOST`\n",
    "* __python version__ - Python 3.7 is the only version of Python available for training and online prediction with runtime version 2.1. (The one we are using.)\n",
    "\n",
    "Note: Runtime version 2.1 uses XGBoost 0.9. Please refer to the [runtime version dependency list](https://cloud.google.com/ml-engine/docs/runtime-version-list).\n",
    "\n",
    "Note: It can take several minutes for you model to be available."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "!gcloud ai-platform versions create $VERSION_NAME \\\n",
    "  --model $MODEL_NAME \\\n",
    "  --origin gs://${BUCKET_PATH}/model/ \\\n",
    "  --runtime-version $RUNTIME_VERSION \\\n",
    "  --python-version $PYTHON_VERSION \\\n",
    "  --framework $FRAMEWORK"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "Creating version (this might take a few minutes)......done.                    \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 6: Make an online prediction\n",
    "\n",
    "It’s time to make  prediction with your newly deployed model. For making the online predictions we will be using the json array that we created. There are two ways demonstrated to make online predictions: using Gcloud and using Python."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use Google Cloud to make online predictions\n",
    "Use the two people (as seen in the table) gathered in the previous step for the gcloud predictions.\n",
    "\n",
    "| **Person** | age | workclass | fnlwgt | education | education-num | marital-status | occupation |\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:\n",
    "| **1** | 25| 4 | 226802 | 1 | 7 | 4 | 7 |\n",
    "| **2** | 28| 2 | 336951 | 7 | 12 | 2 | 11 |\n",
    "\n",
    "| **Person** | relationship | race | sex | capital-gain | capital-loss | hours-per-week | native-country || (Label) income|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||:-:\n",
    "| **1** | 3 | 2 | 1 | 0 | 0 | 40 | 38 || False (<=50K) |\n",
    "| **2** | 0 | 4 | 1 | 0 | 0 | 40 | 38 || True (>50K) |\n",
    "\n",
    "\n",
    "Creating a model version can take several minutes, check the status of your model version to see if it is available."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "! gcloud ai-platform versions list --model $MODEL_NAME"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "NAME  DEPLOYMENT_URI                          STATE\n",
      "v1    gs://maven-user1/census-income/model/  READY\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test the model with an online prediction using the data of a person who makes <=50K.\n",
    "\n",
    "Note: If you see an error, the model from Part 4 may not be created yet as it takes several minutes for a new model version to be created."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use the command line to make online predictions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "! gcloud ai-platform predict --model $MODEL_NAME --version $VERSION_NAME --json-instances less_than_50K.json"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "[0.004633505828678608]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test the model with an online prediction using the data of a person who makes >50K."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "! gcloud ai-platform predict --model $MODEL_NAME --version $VERSION_NAME --json-instances more_than_50K.json"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using endpoint [https://ml.googleapis.com/]\n",
      "[0.3486039638519287]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Realise how the cells above return floats instead of booleans. Let's deal with that below so the output type of the predictions match those of the test set labels. We'll set the prediction to True if it's greater than 0.5 and to False otherwise."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use Python to make online predictions\n",
    "We'll test the model with the entire test set and print out some of the results.\n",
    "\n",
    "Note: If you are running notebook server on Compute Engine, make sure to [\"allow full access to all Cloud APIs\"](https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#changeserviceaccountandscopes)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "import googleapiclient.discovery\n",
    "import os\n",
    "\n",
    "PROJECT_ID = os.environ['PROJECT_ID']\n",
    "VERSION_NAME = os.environ['VERSION_NAME']\n",
    "MODEL_NAME = os.environ['MODEL_NAME']\n",
    "\n",
    "service = googleapiclient.discovery.build('ml', 'v1')\n",
    "name = 'projects/{}/models/{}'.format(PROJECT_ID, MODEL_NAME)\n",
    "name += '/versions/{}'.format(VERSION_NAME)\n",
    "\n",
    "response = service.projects().predict(\n",
    "    name=name,\n",
    "    body={'instances': data}\n",
    ").execute()\n",
    "\n",
    "if 'error' in response:\n",
    "    print (response['error'])\n",
    "else:\n",
    "    online_results = response['predictions']\n",
    "    # convert floats to booleans\n",
    "    converted_responses = [x > 0.5 for x in online_results]\n",
    "    # Print the first 10 responses\n",
    "    for i, response in enumerate(converted_responses[:5]):\n",
    "        print('Prediction: {}\\tLabel: {}'.format(response, test_labels[i]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prediction: False\tLabel: False\n",
      "Prediction: False\tLabel: False\n",
      "Prediction: False\tLabel: True\n",
      "Prediction: True\tLabel: True\n",
      "Prediction: False\tLabel: False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# [Optional] Part 7: Verify Results\n",
    "Let's visualise our predictions with a confusion matrix."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "actual = pd.Series(test_labels, name='actual')\n",
    "online = pd.Series(converted_responses, name='online')\n",
    "\n",
    "pd.crosstab(actual,online)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "online  False  True \n",
       "actual              \n",
       "False   11790    645\n",
       "True     1451   2395"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>online</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>11790</td>\n",
       "      <td>645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1451</td>\n",
       "      <td>2395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's compare this with the confusion matrix of our local model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "bst = xgb.Booster()  # init model\n",
    "bst.load_model('./model.bst')  # load data\n",
    "\n",
    "dtest = xgb.DMatrix(test_features)\n",
    "local_results = bst.predict(dtest)\n",
    "converted_local = [x > 0.5 for x in local_results]\n",
    "local = pd.Series(converted_local, name='local')\n",
    "\n",
    "pd.crosstab(actual, local)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "local   False  True \n",
       "actual              \n",
       "False   11790    645\n",
       "True     1451   2395"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>local</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>11790</td>\n",
       "      <td>645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1451</td>\n",
       "      <td>2395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Better, let's compare the raw results (pre-boolean-conversion) of our local and online models."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "identical = 0\n",
    "different = 0\n",
    "\n",
    "for i in range(len(online_results)):\n",
    "    if online_results[i] == local_results[i]:\n",
    "        identical += 1\n",
    "    else:\n",
    "        different += 1\n",
    "        \n",
    "print('Identical: {}, Different: {}'.format(identical, different))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Identical: 16281, Different: 0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If all results are identical, it means we've successfully uploaded our local model to AI Platform and performed online predictions correctly."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m58"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}